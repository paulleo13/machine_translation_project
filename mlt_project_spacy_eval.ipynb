{"cells":[{"cell_type":"markdown","metadata":{"id":"iAqyPFwqiKmO"},"source":["Hier schmeißen wir die komplette spacy library auf unsere Sätze und werten einfach mal alles aus. \n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40982,"status":"ok","timestamp":1655202847863,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"IN6sybHqhcrv","outputId":"044d33a0-6331-4905-b518-bdc0bcd26904"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (62.4.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0mTraceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 205, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 342, in run\n","    reqs, check_supported_wheels=not options.target_dir\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n","    collected.requirements, max_rounds=try_to_avoid_resolution_too_deep\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\n","    state = resolution.resolve(requirements, max_rounds=max_rounds)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 348, in resolve\n","    self._add_to_criteria(self.state.criteria, r, parent=None)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\n","    if not criterion.candidates:\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n","    return bool(self._sequence)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n","    return any(self)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n","    return (c for c in iterator if id(c) not in self._incompatible_ids)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 90, in _iter_built_with_inserted\n","    for version, func in infos:\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 297, in iter_index_candidate_infos\n","    hashes=hashes,\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 894, in find_best_candidate\n","    candidates = self.find_all_candidates(project_name)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 835, in find_all_candidates\n","    page_candidates = list(page_candidates_it)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/sources.py\", line 134, in page_candidates\n","    yield from self._candidates_from_page(self._link)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 804, in process_project_url\n","    links=page_links,\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 782, in evaluate_links\n","    candidate = self.get_install_candidate(link_evaluator, link)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 763, in get_install_candidate\n","    result, detail = link_evaluator.evaluate_link(link)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 247, in evaluate_link\n","    logger.debug(\"Found link %s, version: %s\", link, version)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1366, in debug\n","    self._log(DEBUG, msg, args, **kwargs)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n","    self.handle(record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n","    self.callHandlers(record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n","    hdlr.handle(record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n","    self.emit(record)\n","  File \"/usr/lib/python3.7/logging/handlers.py\", line 71, in emit\n","    logging.FileHandler.emit(self, record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1127, in emit\n","    StreamHandler.emit(self, record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1025, in emit\n","    msg = self.format(record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 869, in format\n","    return fmt.format(record)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n","    formatted = super().format(record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 608, in format\n","    record.message = record.getMessage()\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 369, in getMessage\n","    msg = msg % self.args\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/models/link.py\", line 93, in __str__\n","    redact_auth_from_url(self._url), self.comes_from, rp\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/misc.py\", line 525, in redact_auth_from_url\n","    return _transform_url(url, _redact_netloc)[0]\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/misc.py\", line 490, in _transform_url\n","    purl = urllib.parse.urlsplit(url)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n","    return command.main(cmd_args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n","    return self._main(args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 221, in _main\n","    return run(options, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 204, in exc_logging_wrapper\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1425, in critical\n","    self._log(CRITICAL, msg, args, **kwargs)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n","    self.handle(record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n","    self.callHandlers(record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n","    hdlr.handle(record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n","    self.emit(record)\n","  File \"/usr/lib/python3.7/logging/handlers.py\", line 71, in emit\n","    logging.FileHandler.emit(self, record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1127, in emit\n","    StreamHandler.emit(self, record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1025, in emit\n","    msg = self.format(record)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 869, in format\n","    return fmt.format(record)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 118, in format\n","    prefix = f\"{self.formatTime(record)} \"\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 550, in formatTime\n","    t = time.strftime(self.default_time_format, ct)\n","KeyboardInterrupt\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.3.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (62.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.17)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.3.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (62.4.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.5.18.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting es-core-news-sm==3.3.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.3.0/es_core_news_sm-3.3.0-py3-none-any.whl (12.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.3.0) (3.3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.11.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.9.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.2)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.6.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (62.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.6)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.4.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.8.2)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.1.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.7)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.9)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.4.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (21.3)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.21.6)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.3.0)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (8.0.17)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.64.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2022.5.18.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install -U pip setuptools wheel\n","!pip install -U spacy\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download es_core_news_sm"]},{"cell_type":"code","source":["# show cpu specs \n","# !cat /proc/cpuinfo\n","# !cat /proc/meminfo"],"metadata":{"id":"52EOcdvwh400"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4600,"status":"ok","timestamp":1655202852455,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"rMXkKHUvkCbs"},"outputs":[],"source":["import spacy\n","from spacy.lang.en import English\n","from spacy.lang.es import Spanish\n","from spacy import tokenizer\n","from spacy.tokens import DocBin"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1655202852456,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"GV0CdON14fyP"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import time\n","import glob"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1655202852457,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"iSSDgbTsnM-h"},"outputs":[],"source":["# other imports\n","import os\n","from os import path\n","from google.colab import drive"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2060,"status":"ok","timestamp":1655202854507,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"WZVeyRc9nYPV","outputId":"0325503d-82a1-404a-a12b-77b6bbcd20bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# mount google drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1315,"status":"ok","timestamp":1655202855816,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"mbH-xRMwk2eg"},"outputs":[],"source":["nlp_en = spacy.load(\"en_core_web_sm\")\n","nlp_es = spacy.load(\"es_core_news_sm\")"]},{"cell_type":"code","source":["# process more shit in parallel \n","def read_data(inputfile):\n","    lines = load_data(inputfile)\n","    df = pd.DataFrame(lines, columns=['content'])\n","    return df\n","\n","\n","def tag_dep_pipe(doc):\n","    tag_dep_list = [[tok.text, tok.lemma_, tok.pos_, tok.tag_, tok.dep_] for tok in doc\n","                  if tok.is_alpha and not tok.is_stop]\n","    return tag_dep_list\n","\n","\n","def preprocess_pipe(texts):\n","    preproc_pipe = []\n","    for doc in nlp_en.pipe(texts, batch_size=2000):\n","        preproc_pipe.append(tag_dep_pipe(doc))\n","    return preproc_pipe\n","\n","\n","def chunker(iterable, total_length, chunksize):\n","    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n","\n","\n","def flatten(list_of_lists):\n","    \"Flatten a list of lists to a combined list\"\n","    return [item for sublist in list_of_lists for item in sublist]\n","\n","\n","def process_chunk(texts):\n","    preproc_pipe = []\n","    for doc in nlp_en.pipe(texts, batch_size=2000):\n","        preproc_pipe.append(tag_dep_pipe(doc))\n","    return preproc_pipe\n","\n","\n","def preprocess_parallel(texts, chunksize=100):\n","    executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"processes\")\n","    do = delayed(process_chunk)\n","    tasks = (do(chunk) for chunk in chunker(texts, len(df_preproc), chunksize=chunksize))\n","    result = executor(tasks)\n","    return flatten(result)\n","\n","\n","df_preproc['preproc_parallel'] = preprocess_parallel(df_preproc['clean'], chunksize=10000)"],"metadata":{"id":"HpzUkp20ioa-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5auQCcbj5K9g"},"outputs":[],"source":["# using spacy DocBin \n","doc_bin_en = DocBin()\n","trl_data = load_data(translated_en_path)\n","print(trl_data[0])\n","for i, doc in enumerate(nlp_en.pipe(\" \".join(trl_data), batch_size=2000)):\n","  print(\"Current batch is \", i)\n","  doc_bin_en.add(doc)\n","bytes_data = doc_bin_en.to_bytes()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5S3lm2hn9E4g"},"outputs":[],"source":["with open(path.join(eval_path, \"doc_bin_container_en\"), 'wb') as f:\n","  f.write(bytes_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8HrxS4kXOpbO"},"outputs":[],"source":["def pipe_en_translations():\n","  doc_bin_en = DocBin()\n","  trl_data = load_data(translated_en_path)\n","  for number, data in enumerate():\n","    for i, doc in enumerate(nlp_en.pipe(\" \".join(trl_data), batch_size=5000)):\n","      print(\"Current batch is \", i)\n","      doc_bin_en.add(doc)\n","    bytes_data = doc_bin_en.to_bytes()\n","    with open(path.join(eval_path, f'doc_bin_container_en_{number}'), 'wb') as f:\n","      f.write(bytes_data)\n","\n","\n","def pipe_es_translations(translations):\n","  doc_bin_en = DocBin()\n","  trl_data = load_data(translated_en_path)\n","  for number, data in enumerate():\n","    for i, doc in enumerate(nlp_es.pipe(\" \".join(trl_data), batch_size=5000)):\n","      print(\"Current batch is \", i)\n","      doc_bin_en.add(doc)\n","    bytes_data = doc_bin_en.to_bytes()\n","    with open(path.join(eval_path, f'doc_bin_container_es_{number}'), 'wb') as f:\n","      f.write(bytes_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2CoWEza9Cd1"},"outputs":[],"source":["# deserialize later\n","doc_bin_path = path.join(eval_path, \"doc_bin_container_en\")\n","doc_bin_deserialized = DocBin().from_bytes(doc_bin_path)\n","docs = list(doc_bin_deserialized.get_docs(nlp_en.vocab))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":578,"status":"ok","timestamp":1655202881511,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"J6ycUHnSmLfw"},"outputs":[],"source":["# load data function\n","def load_data(file_path):\n","  try:\n","    with open(file_path, \"r\") as f:\n","      lines = f.readlines()\n","      return [s.replace(\"\\n\", \"\") for s in lines]\n","  except EnvironmentError as e:\n","      print(\"Could not read file because exception occured: \", e.with_traceback())"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1655202882959,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"dBkWaOimyNJb"},"outputs":[],"source":["# divide data into batches\n","def create_batches(data, batchsize):\n","  \"\"\"Divides a batch of data into batches of given size.\n","\n","  Args:\n","    data: The data to split in list format.\n","    batchsize: Size of desired batches.\n","\n","  Returns:\n","    Iterable of all batches with batchsize.\n","  \n","  Raises:\n","    AssertionError if the size of the data is not a multiple of batchsize.\n","\n","  \"\"\"\n","  assert len(data) % batchsize == 0\n","  batches = []\n","  num_batches = len(data) // batchsize\n","  for i in range(num_batches):\n","    batches.append(data[i*batchsize:(i+1)*batchsize])\n","  return batches"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655147070195,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"l6OG2DXimkrX"},"outputs":[],"source":["# data paths\n","translated_path = \"/content/drive/MyDrive/mlt_final_project/translated\"\n","preprocessed_path = \"/content/drive/MyDrive/mlt_final_project/preprocessed\"\n","it_trl_path = \"/content/drive/MyDrive/mlt_final_project/iterative_translation\"\n","eval_path = path.join(\"/content/drive/MyDrive/mlt_final_project\", \"evaluation\")\n","translated_en_path = path.join(translated_path, \"de-en\", \"translated.en\")\n","translated_es_path = path.join(translated_path, \"de-es\", \"translated.es\")\n","reference_en_path = path.join(preprocessed_path, \"de-en\", \"pp_deen.en\")\n","reference_es_path = path.join(preprocessed_path, \"de-es\", \"pp_dees.es\")\n","# translation files from english to german\n","it_trl_ende_paths = [path.join(it_trl_path, f'backward_trl_ende_{num}.txt') for num in range(5)]\n","# translation files from german to english\n","it_trl_deen_paths = [path.join(it_trl_path, f'forward_trl_deen_{num}.txt') for num in range(5)]\n","# translation files from spanish to german\n","it_trl_esde_paths = [path.join(it_trl_path, f'backward_trl_esde_{num}.txt') for num in range(5)]\n","# translation files from german to spanish\n","it_trl_dees_paths = [path.join(it_trl_path, f'forward_trl_dees_{num}.txt') for num in range(5)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lqpu7gS_tBVY"},"outputs":[],"source":["# load data\n","trl_data = load_data(translated_en_path) #, translated_es_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VylKHqhxxpFv"},"outputs":[],"source":["# evaluation\n","trl_en_batches = create_batches(trl_data[0], 1000)\n","trl_es_batches = create_batches(trl_data[1], 1000)\n","# len(trl_en_batches)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1654543283526,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"iccdmzbKry2T","outputId":"f3dd7b33-f57f-4f94-da6f-441df4abefa8"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'spacy.tokens.doc.Doc'>\n"]}],"source":["# for batch in trl_en_batches:\n","#   trl_doc_en = nlp_en(\" \".join(trl_data[0]))\n","#   trl_doc_es = nlp_es(\" \".join(trl_data[1]))\n","trl_doc_en = nlp_en(\" \".join(trl_data[0][:2]))"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":380,"status":"ok","timestamp":1655204443933,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"y1YJnDvd2WAJ"},"outputs":[],"source":["def token_extraction(spacy_nlp_obj):\n","  tokens = []\n","  for token in spacy_nlp_obj:\n","    tokens.append([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n","            token.shape_, token.is_alpha, token.is_stop])\n","  return tokens\n","\n","def create_token_dataframes(trl_batch, language, name):\n","  # test_path = path.join(\"/content/drive/MyDrive/mlt_final_project\", \"test\")\n","  dataframes = []\n","  for number, batch in enumerate(trl_batch):\n","    start = time.time()\n","    print(f'Processing batch number {number+1} of {len(trl_batch)}...')\n","    if language == \"en\":\n","      trl_nlp_obj = nlp_en(\" \".join(batch))\n","      # trl_nlp_obj.to_disk(path.join(spacy_models_path, \"en\", f'en_spacy_model_{name}_{number}'))\n","    else:\n","      trl_nlp_obj = nlp_es(\" \".join(batch))\n","      # trl_nlp_obj.to_disk(path.join(spacy_models_path, \"es\", f'es_spacy_model_{name}_{number}'))\n","    tokens = token_extraction(trl_nlp_obj)\n","    df = pd.DataFrame(tokens, columns=['Text', 'Lemma', 'Pos', 'Tag', 'Dep', 'Shape', 'Alpha', 'Stop'])\n","    # df.to_csv(f'{test_path}/{name}_{number}.csv')\n","    dataframes.append(df)\n","    end = time.time()\n","    print(f'Processing batch number {number+1} took {end-start} seconds.')\n","  return dataframes"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1655202888961,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"t4pchd9C8k9S"},"outputs":[],"source":["def concatenate_df_batches(df_batches: list):\n","  return pd.concat(df_batches, ignore_index=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655202890246,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"B7KnA1qSXoY-"},"outputs":[],"source":["# ['Text', 'Lemma', 'Pos', 'Tag', 'Dep', 'Shape', 'Alpha', 'Stop']\n"," \n","# count hapax legomena, bi legomena tri legomena\n","def count_legomas(df):\n","  words, word_counts = np.unique(df['Text'].to_numpy(), return_counts=True)\n","  hapax_legoma = words[word_counts == 1]\n","  bi_legoma = words[word_counts == 2]\n","  tri_legoma = words[word_counts == 3]\n","  return hapax_legoma.size, bi_legoma.size, tri_legoma.size\n","\n","\n","# filters stop characters and non alphanumericals from df\n","def filter_stop_non_alpha(df):\n","  filtered_df = df.loc[(df['Stop'] == False) & (df['Alpha'] == True)]\n","  return filtered_df.drop(columns=['Stop', 'Alpha'])"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":470,"status":"ok","timestamp":1655202892506,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"FPScJL6mDO-3"},"outputs":[],"source":["# shannon entropy\n","def shannon_entropy_o(dataframe):\n","  lemma_frames = dataframe.groupby('Lemma')\n","  entropy_values=[]\n","  for lemma, frame in lemma_frames:\n","    words, word_counts = np.unique(frame['Text'].to_numpy(), return_counts=True)\n","    normalized_word_counts = word_counts / np.sum(word_counts)\n","    entropy = -np.sum(normalized_word_counts * np.log(normalized_word_counts))\n","    entropy_values.append(entropy)\n","  return np.mean(entropy_values)\n","\n","\n","# Simpson's diversity\n","def simpson_diversity_o(dataframe):\n","  lemma_frames = dataframe.groupby('Lemma')\n","  diversity_values=[]\n","  for lemma, frame in lemma_frames:\n","    words, word_counts = np.unique(frame['Text'].to_numpy(), return_counts=True)\n","    normalized_word_counts = word_counts / np.sum(word_counts)\n","    diversity = 1/np.sum(normalized_word_counts**2)\n","    diversity_values.append(diversity)\n","  return np.mean(diversity_values)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655202893587,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"ZRHDN4y-bON-"},"outputs":[],"source":["# Lexical Frequency Profile\n","def get_lfp_o(data_frame, return_index_dict=False):\n","  words, word_counts = np.unique(data_frame['Text'].to_numpy(), return_counts=True)\n","  sorted_indices = np.argsort(word_counts)\n","  # potential to get the actual most used words\n","  sorted_words = words[sorted_indices]\n","  sorted_word_counts = word_counts[sorted_indices]\n","  denominator = np.sum(sorted_word_counts)\n","  first_thousand_percentage = np.sum(sorted_word_counts[:1000])/denominator\n","  second_thousand_percentage = np.sum(sorted_word_counts[1000:2000])/denominator\n","  rest_percentage = np.sum(sorted_word_counts[2000:])/denominator\n","  index_dict = None\n","  if return_index_dict:\n","    for word, index in zip(sorted_words, sorted_indices):\n","      index_dict[word] = index\n","  return [first_thousand_percentage, second_thousand_percentage, rest_percentage], index_dict"]},{"cell_type":"code","source":["# nici functions\n","def ttr_ratio(dataframe, lemma=False, sort_punctuation_out=True, sort_stopwords_out=False):\n","  if sort_stopwords_out:\n","      dataframe = dataframe.loc[dataframe['Stop'] == False]\n","  if sort_punctuation_out:\n","      dataframe = dataframe.loc[dataframe['Tag'] != 'PUNCT']\n","  if lemma:\n","    words = dataframe[\"Lemma\"].to_numpy()\n","  else:\n","    words = dataframe[\"Text\"].to_numpy()\n","  return len(np.unique(words))/len(words)\n","\n","\n","def shannon_entropy(dataframe, normalizing_constant=None):\n","  if normalizing_constant is None:\n","    normalizing_constant = 0\n","    count=True\n","  else:\n","    count=False\n","  if 'relevant_lemma' in dataframe.columns:\n","    subframe = dataframe.loc[dataframe['relevant_lemma'] == True]\n","  else:\n","    subframe = dataframe\n","  lemma_frames = subframe.groupby('Lemma')\n","  entropy_values=[]\n","  for lemma, frame in lemma_frames:\n","    #print(frame)\n","    #print(\".\")\n","    words, word_counts = np.unique(frame['Text'].to_numpy(), return_counts=True)\n","    # TODO: decide whether we want to remove all words that have in theory more \n","    # than one lemma but not in our data\n","    if len(words) < 2:\n","      continue\n","    if count:\n","      normalizing_constant += 1\n","    normalized_word_counts = word_counts / np.sum(word_counts)\n","    entropy = -np.sum(normalized_word_counts * np.log(normalized_word_counts))\n","    entropy_values.append(entropy) \n","  return np.sum(entropy_values)/normalizing_constant, normalizing_constant\n","\n","\n","def simpson_diversity(dataframe):\n","  if 'relevant_lemma' in dataframe.columns:\n","    subframe = dataframe.loc[dataframe['relevant_lemma'] == True]\n","  else:\n","    subframe = dataframe\n","  lemma_frames = subframe.groupby('Lemma')\n","  diversity_values=[]\n","  for lemma, frame in lemma_frames:\n","    if len(frame) == 1:\n","      continue\n","    words, word_counts = np.unique(frame['Text'].to_numpy(), return_counts=True)\n","    # TODO: decide whether we want to remove all words that have in theory more \n","    # than one lemma but not in our data\n","    if len(words) < 2:\n","      continue\n","    normalized_word_counts = word_counts / np.sum(word_counts)\n","    diversity = np.sum(normalized_word_counts**2)\n","    diversity_values.append(diversity)\n","  return np.mean(diversity_values)\n","\n","\n","def get_lfp(data_frame, index_dict=None, return_index_dict=False, sort_punctuation_out=True, sort_stopwords_out=False):\n","    if sort_stopwords_out:\n","      data_frame = data_frame.loc[data_frame['Stop'] == False]\n","    if sort_punctuation_out:\n","      data_frame = data_frame.loc[data_frame['Tag'] != 'PUNCT']\n","\n","    words, word_counts = np.unique(data_frame['Lemma'].to_numpy(), return_counts=True)\n","    if index_dict is not None:\n","      new_word_indices = []\n","      restructured_indices = []\n","      for word in words:\n","        if word in index_dict.keys():\n","          restructured_indices.append(index_dict[word])\n","          new_word_indices.append(False)\n","        else:\n","          new_word_indices.append(True)\n","      sorted_word_counts = word_counts[restructured_indices]          \n","      new_words = words[new_word_indices]\n","      new_word_counts = word_counts[new_word_indices]\n","      sorted_word_counts = np.append(sorted_word_counts, new_word_counts)\n","      denominator = np.sum(sorted_word_counts)\n","      first_thousand_percentage = np.sum(sorted_word_counts[:1000])/denominator\n","      second_thousand_percentage = np.sum(sorted_word_counts[1000:2000])/denominator\n","      rest_percentage = np.sum(sorted_word_counts[2000:])/denominator\n","      new_word_percentage = np.sum(new_word_counts)/denominator\n","      index_dict = None\n","      return [first_thousand_percentage, second_thousand_percentage, rest_percentage], index_dict  \n","    else:\n","      sorted_indices = np.argsort(word_counts)[::-1]\n","      # potential to get the actual most used words\n","      sorted_words = words[sorted_indices]\n","      sorted_word_counts = word_counts[sorted_indices]\n","      denominator = np.sum(sorted_word_counts)\n","      #print(sorted_word_counts)\n","      first_thousand_percentage = np.sum(sorted_word_counts[:1000])/denominator\n","      second_thousand_percentage = np.sum(sorted_word_counts[1000:2000])/denominator\n","      rest_percentage = np.sum(sorted_word_counts[2000:])/denominator\n","      index_dict = None\n","      if return_index_dict:\n","        index_dict = {}\n","        for  index, word in enumerate(sorted_words[:2000]):\n","          index_dict[word] = index\n","      return [first_thousand_percentage, second_thousand_percentage, rest_percentage], index_dict"],"metadata":{"id":"C-PAichoFCzZ","executionInfo":{"status":"ok","timestamp":1655209217112,"user_tz":-120,"elapsed":625,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["dfs_path = path.join(\"/content/drive/MyDrive/mlt_final_project\", \"dfs\")\n","paths = glob.glob(f'{dfs_path}/*')\n","paths"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55X2RgQwHK1K","executionInfo":{"status":"ok","timestamp":1655208513783,"user_tz":-120,"elapsed":456,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"}},"outputId":"0c141833-ea61-4ce0-b3ed-b9915156f43b"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/mlt_final_project/dfs/pp_deen.en',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_deen_1',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_deen_0',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_deen_2',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_deen_3',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/translated.en',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/pp_dees.es',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_1',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_0',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_2',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_3',\n"," '/content/drive/MyDrive/mlt_final_project/dfs/translated.es']"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":512,"status":"ok","timestamp":1655209631109,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"_d6rEOE6Rq35"},"outputs":[],"source":["def evaluate(path_obj, language, name, spacy_model_from_disk=False, only_specific_models=False, number_of_batch=None, start_batch=False, end_batch=False):\n","  batches = [] \n","  if only_specific_models:\n","    batches = only_load_specific_batches(path_obj, start_batch=False, num_batch=number_of_batch, end_batch=False)\n","  else: \n","    # load data\n","    data = load_data(path_obj)\n","    # batch data\n","    batches = create_batches(data, 1000)\n","  # run spacy tokenizer and classifier\n","  dfs = create_token_dataframes(batches, language, name)\n","  # pack all into one df\n","  df = concatenate_df_batches(dfs)\n","  # filter stop and non alphanumericals\n","  clean_df = filter_stop_non_alpha(df)\n","  try:\n","    # save entire df to disk\n","    superclean_df = clean_df.dropna(how='any',axis=0)  \n","    dfs_path = path.join(\"/content/drive/MyDrive/mlt_final_project\", \"dfs\")\n","    superclean_df.to_csv(path.join(dfs_path, name))\n","  except EnvironmentError as e:\n","    print(e.with_traceback())\n","  # simpsons diversity\n","  sim_div = simpson_diversity(clean_df)\n","  # shannon entropy\n","  shan_entr = shannon_entropy(clean_df)\n","  # lexical frequency\n","  lexical_freq = get_lfp(clean_df)\n","  # legoma count -> hapax legomenon, bi legomenon tri legomenon\n","  legomas = count_legomas(clean_df)\n","  # lemma count -> filtered stop words and non alphanumericals\n","  lemma_dict = {}\n","  struct_dict = {}\n","  dep_dict = {}\n","  for _, row in clean_df.iterrows():\n","    if row['Lemma'] not in lemma_dict.keys():\n","      lemma_dict.update({row['Lemma']: 0})\n","    else:\n","      lemma_dict[row['Lemma']] += 1\n","  # noun, verb, adjective, adverb, sub. conjunction count\n","    if row['Pos'] not in struct_dict.keys():\n","      struct_dict.update({row['Pos']: 0})\n","    else:\n","      struct_dict[row['Pos']] += 1\n","  # dependency count\n","    if row['Dep'] not in dep_dict.keys():\n","      dep_dict.update({row['Dep']: 0})\n","    else:\n","      dep_dict[row['Dep']] += 1\n","\n","  return sim_div, shan_entr, lexical_freq, legomas, lemma_dict, struct_dict, dep_dict\n","\n","#    \n","def load_and_evaluate_df():\n","  # metrics_list = []\n","  # dfs_path = path.join(\"/content/drive/MyDrive/mlt_final_project\", \"dfs\")\n","  paths = ['/content/drive/MyDrive/mlt_final_project/dfs/pp_dees.es', \n","           '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_1', \n","           '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_0',\n","           '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_2',\n","           '/content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_3',\n","           '/content/drive/MyDrive/mlt_final_project/dfs/translated.es']\n","  norm_const = None\n","  index_dict = None\n","  for f_path in paths:\n","    print(f'Loading {f_path}')\n","    output_path = f_path.replace(\".csv\", \"\").split(\"/\")[-1]\n","    df = pd.read_csv(f_path, header=0)\n","    clean_df = df.dropna(how='any',axis=0)\n","    print(f'Loaded {f_path}')\n","    print(clean_df.loc[0,:])\n","    print(f'DataFrame has Null values: {clean_df.isnull().values.any()}')\n","    # type token ratio\n","    ttr_words = ttr_ratio(clean_df, lemma=False, sort_punctuation_out=True, sort_stopwords_out=False)\n","    # type token ratio lemmas\n","    ttr_lemmas = ttr_ratio(clean_df, lemma=True, sort_punctuation_out=True, sort_stopwords_out=False)\n","    # simpson diversity\n","    sim_div = simpson_diversity(clean_df)\n","    if \"pp_deen\" in f_path:\n","      # shannon entropy\n","      shan_entr, norm_const_ = shannon_entropy(clean_df, normalizing_constant=None)  # (dataframe, normalizing_constant=None)\n","      norm_const = norm_const_\n","    else:\n","      shan_entr = shannon_entropy(clean_df, normalizing_constant=norm_const)  # (dataframe, normalizing_constant=None) \n","    if \"pp_deen\" in f_path:\n","      # lexical frequency\n","      lexical_freq = get_lfp(clean_df, index_dict=None) # (data_frame, index_dict=None, return_index_dict=False, sort_punctuation_out=True, sort_stopwords_out=False)\n","      index_dict = lexical_freq[1]\n","    else:\n","      # lexical frequency\n","      lexical_freq = get_lfp(clean_df, index_dict=index_dict) # (data_frame, index_dict=None, return_index_dict=False, sort_punctuation_out=True, sort_stopwords_out=False)\n","    # legoma count -> hapax legomenon, bi legomenon tri legomenon\n","    legomas = count_legomas(clean_df)\n","    # lemma count -> filtered stop words and non alphanumericals\n","    lemma_dict = {}\n","    struct_dict = {}\n","    dep_dict = {}\n","    for _, row in clean_df.iterrows():\n","      if row['Lemma'] not in lemma_dict.keys():\n","        lemma_dict.update({row['Lemma']: 0})\n","      else:\n","        lemma_dict[row['Lemma']] += 1\n","    # noun, verb, adjective, adverb, sub. conjunction count\n","      if row['Pos'] not in struct_dict.keys():\n","        struct_dict.update({row['Pos']: 0})\n","      else:\n","        struct_dict[row['Pos']] += 1\n","    # dependency count\n","      if row['Dep'] not in dep_dict.keys():\n","        dep_dict.update({row['Dep']: 0})\n","      else:\n","        dep_dict[row['Dep']] += 1\n","    process_created_dfs(output_path, ttr_words, ttr_lemmas, sim_div, shan_entr, lexical_freq, legomas, lemma_dict, struct_dict, dep_dict)\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"8Ln7u0tlCTiE","executionInfo":{"status":"ok","timestamp":1655147132210,"user_tz":-120,"elapsed":337,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"}}},"outputs":[],"source":["# load saved spacy models from disk to continue evaluation\n","def load_saved_spacy_model_to_df(language):\n","  spacy_models_path = path.join(\"/content/drive/MyDrive/mlt_final_project\", \"spacy_models\")\n","  first_it_model_paths = glob.glob(f'{spacy_models_path}/{language}_old/{language}_spacy_model_forward_trl_de{language}_0_*')\n","  sec_it_model_paths = glob.glob(f'{spacy_models_path}/{language}_old/{language}_spacy_model_forward_trl_de{language}_1_*')\n","  third_it_model_paths = glob.glob(f'{spacy_models_path}/{language}_old/{language}_spacy_model_forward_trl_de{language}_2_*')\n","  fourth_it_model_paths = glob.glob(f'{spacy_models_path}/{language}_old/{language}_spacy_model_forward_trl_de{language}_3_*')\n","  first_it_models = []\n","  sec_it_models = []\n","  third_it_models = []\n","  fourth_it_models = []\n","  if \"es\" in language:\n","    first_it_models = [nlp_es.from_disk(model) for model in first_it_model_paths]\n","    sec_it_models = [nlp_es.from_disk(model) for model in sec_it_model_paths]\n","    third_it_models = [nlp_es.from_disk(model) for model in third_it_model_paths]\n","    fourth_it_models = [nlp_es.from_disk(model) for model in fourth_it_model_paths]\n","  else:\n","    first_it_models = [nlp_en.from_disk(model) for model in first_it_model_paths]\n","    sec_it_models = [nlp_en.from_disk(model) for model in sec_it_model_paths]\n","    third_it_models = [nlp_en.from_disk(model) for model in third_it_model_paths]\n","    fourth_it_models = [nlp_en.from_disk(model) for model in fourth_it_model_paths]\n","\n","  "]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":449,"status":"ok","timestamp":1655202903274,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"ty7Tp7hAkgy_"},"outputs":[],"source":["def only_load_specific_batches(path_obj, start_batch=False, num_batch=0, end_batch=True):\n","  # load data\n","  data = load_data(path_obj)\n","  # batch data\n","  batches = create_batches(data, 1000)\n","  if start_batch:\n","    return batches[num_batch:]\n","  elif end_batch:\n","    return batches[num_batch-1:num_batch]\n","  else:\n","    return batches"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":376,"status":"ok","timestamp":1655209227318,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"uq8gfHb8GHQi"},"outputs":[],"source":["# writes the given parameters (evaluation metrics) to disk as .csv and .txt files\n","def write_metrics_to_disk(ttr_words, ttr_lemmas, sim_div, shan_entr, lexical_freq, legomas, lemma_dict, struct_dict, dep_dict, file_path):\n","  #sum_structs = sum([value for key, value in struct_dict.items()])\n","  if os.path.exists(f'{file_path}.txt'):\n","    append_write = 'a' # append if already exists\n","  else:\n","    append_write = 'w' # make a new file if not\n","  if os.path.exists(f'{file_path}.csv'):\n","    append_write_csv = 'a' # append if already exists\n","  else:\n","    append_write_csv = 'w' # make a new file if not\n","  try:\n","    with open(f'{file_path}.txt', append_write) as f:  \n","      f.write(f'- Grammatical structure count -\\n')\n","      #f.write(f'Sum of all structs is {sum_structs}\\n')\n","      #for key, value in struct_dict.items():\n","      #  f.write(f'{key}: {value}\\n')\n","      f.write(f'Nouns: {struct_dict[\"NOUN\"]}, Verbs: {struct_dict[\"VERB\"]}, Adverbs: {struct_dict[\"ADV\"]}, Adjectives: {struct_dict[\"ADJ\"]}\\n')\n","      f.write(f'- Other metrics -\\n')\n","      f.write(f'TTR words is: {ttr_words}\\n')\n","      f.write(f'TTR lemmas is: {ttr_lemmas}\\n')\n","      f.write(f'The shannon entropy is: {shan_entr}\\n')\n","      f.write(f'The simpson diversity is: {sim_div}\\n')\n","      f.write(f'The lexical frequency is: First thousand {lexical_freq[0][0]}, second thousand {lexical_freq[0][1]}, rest {lexical_freq[0][2]}\\n')\n","      f.write(f'The hapax legoma are: {legomas[0]}\\n')\n","      f.write(f'The bi legoma are: {legomas[1]}\\n')\n","      f.write(f'The tri legoma are: {legomas[2]}\\n')\n","      f.write(f'The number of lemmas are: {len(lemma_dict)}\\n')\n","      f.write(f'- Syntactical dependencies -\\n')\n","      for key, value in dep_dict.items():\n","        f.write(f'{key}: {value}\\n')\n","  except EnvironmentError as e:\n","    print(f'Error occured while writing {file_path}.txt')\n","    print(e.with_traceback())\n","\n","  try:\n","    with open(f'{file_path}.csv', append_write_csv) as f:\n","      f.write(f'Nouns, {struct_dict[\"NOUN\"]}\\n')\n","      f.write(f'Verbs, {struct_dict[\"VERB\"]}\\n')\n","      f.write(f'Adverbs, {struct_dict[\"ADV\"]}\\n')\n","      f.write(f'Adjectives, {struct_dict[\"ADJ\"]}\\n')\n","      # f.write(f'Subordinating_conjunctions, {struct_dict[\"SCONJ\"]}\\n')\n","      f.write(f'TTR_words, {ttr_words}\\n')\n","      f.write(f'TTR_lemmas, {ttr_lemmas}\\n')\n","      f.write(f'shannon_entropy, {shan_entr}\\n')\n","      f.write(f'simpson_diversity, {sim_div}\\n')\n","      f.write(f'lexical_frequency_first_thousand, {lexical_freq[0][0]}\\n')\n","      f.write(f'lexical_frequency_second_thousand, {lexical_freq[0][1]}\\n')\n","      f.write(f'lexical_frequency_third_thousand, {lexical_freq[0][2]}\\n')\n","      f.write(f'hapax_legoma, {legomas[0]}\\n')\n","      f.write(f'bi_legoma, {legomas[1]}\\n')\n","      f.write(f'tri_legoma, {legomas[2]}\\n')\n","      f.write(f'lemmas, {len(lemma_dict)}\\n')\n","      for key, value in dep_dict.items():\n","        f.write(f'{key}, {value}\\n')\n","  except EnvironmentError as e:\n","    print(f'Error occured while writing {file_path}.csv')\n","    print(e.with_traceback())"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":348,"status":"ok","timestamp":1655209237115,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"},"user_tz":-120},"id":"76pUvbMhPqof"},"outputs":[],"source":["def process():\n","  it_trl_path = \"/content/drive/MyDrive/mlt_final_project/iterative_translation\"\n","  translated_path = \"/content/drive/MyDrive/mlt_final_project/translated\"\n","  preprocessed_path = \"/content/drive/MyDrive/mlt_final_project/preprocessed\"\n","  # translation files from german to english\n","  it_trl_deen_paths = [path.join(it_trl_path, f'forward_trl_deen_{num}.txt') for num in range(4)]\n","  # translation files from german to spanish\n","  it_trl_dees_paths = [path.join(it_trl_path, f'forward_trl_dees_{num}.txt') for num in range(4)]\n","  translated_en_path = path.join(translated_path, \"de-en\", \"translated.en\")\n","  translated_es_path = path.join(translated_path, \"de-es\", \"translated.es\")\n","  reference_en_path = path.join(preprocessed_path, \"de-en\", \"pp_deen.en\")\n","  reference_es_path = path.join(preprocessed_path, \"de-es\", \"pp_dees.es\")\n","  eval_path = path.join(\"/content/drive/MyDrive/mlt_final_project\", \"evaluation_two\")\n","\n","  # path.join(it_trl_path, f'forward_trl_deen_1.txt') von batch 19 bis 50\n","  # path.join(it_trl_path, f'forward_trl_deen_2.txt') ganz\n","  # path.join(it_trl_path, f'forward_trl_deen_3.txt') ganz\n","  # f'forward_trl_dees_3.txt', f'forward_trl_deen_2.txt', f'forward_trl_deen_3.txt'\n","  all_files = [path.join(preprocessed_path, \"de-es\", \"pp_dees.es\"), path.join(it_trl_path, f'forward_trl_dees_1.txt'), path.join(it_trl_path, f'forward_trl_dees_0.txt'), path.join(it_trl_path, f'forward_trl_dees_2.txt'),\n","               path.join(it_trl_path, f'forward_trl_dees_3.txt'), translated_es_path]\n","  for path_obj in all_files:\n","    output_path = path_obj.replace(\".txt\", \"\").split(\"/\")[-1]\n","    # if \"es\" in path_obj and \"pp_deen\" not in path_obj:\n","    sim_div, shan_entr, lexical_freq, legomas, lemma_dict, struct_dict, dep_dict = evaluate(path_obj, \"es\", output_path, only_specific_models=False, number_of_batch=None, start_batch=False, end_batch=False)\n","    # else:\n","    # sim_div, shan_entr, lexical_freq, legomas, lemma_dict, struct_dict, dep_dict = evaluate(path_obj, \"en\", output_path, only_specific_models=False, number_of_batch=None, start_batch=False, end_batch=False)\n","\n","  total_output_path = path.join(eval_path, output_path)\n","  # save all evaluation metrics\n","  write_metrics_to_disk(sim_div, shan_entr, lexical_freq, legomas, lemma_dict, struct_dict, dep_dict, total_output_path)\n","\n","\n","def process_created_dfs(output_path, ttr_words, ttr_lemmas, sim_div, shan_entr, lexical_freq, legomas, lemma_dict, struct_dict, dep_dict):\n","  eval_two_path = path.join(\"/content/drive/MyDrive/mlt_final_project\", \"evaluation_two\")\n","  print(f'Name of file in write method is {output_path}')\n","  output_tot_path = path.join(eval_two_path, output_path)\n","  print(f'Final output path (write method) is {output_tot_path}')\n","  write_metrics_to_disk(ttr_words, ttr_lemmas, sim_div, shan_entr, lexical_freq, legomas, lemma_dict, struct_dict, dep_dict, output_tot_path) "]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ZeZw2szdraX","outputId":"c913adc8-32e9-472d-be39-339240e07f5d","executionInfo":{"status":"ok","timestamp":1655209907018,"user_tz":-120,"elapsed":261368,"user":{"displayName":"Paul Gekeler","userId":"08774043658634399772"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading /content/drive/MyDrive/mlt_final_project/dfs/pp_dees.es\n","Loaded /content/drive/MyDrive/mlt_final_project/dfs/pp_dees.es\n","Unnamed: 0              0\n","Text          Reanudación\n","Lemma         Reanudación\n","Pos                 PROPN\n","Tag                 PROPN\n","Dep                  ROOT\n","Shape               Xxxxx\n","Name: 0, dtype: object\n","DataFrame has Null values: False\n","Name of file in write method is pp_dees.es\n","Final output path (write method) is /content/drive/MyDrive/mlt_final_project/evaluation_two/pp_dees.es\n","Loading /content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_1\n","Loaded /content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_1\n","Unnamed: 0              0\n","Text          Reanudación\n","Lemma         Reanudación\n","Pos                 PROPN\n","Tag                 PROPN\n","Dep                  ROOT\n","Shape               Xxxxx\n","Name: 0, dtype: object\n","DataFrame has Null values: False\n","Name of file in write method is forward_trl_dees_1\n","Final output path (write method) is /content/drive/MyDrive/mlt_final_project/evaluation_two/forward_trl_dees_1\n","Loading /content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_0\n","Loaded /content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_0\n","Unnamed: 0              0\n","Text          Reanudación\n","Lemma         Reanudación\n","Pos                 PROPN\n","Tag                 PROPN\n","Dep                  ROOT\n","Shape               Xxxxx\n","Name: 0, dtype: object\n","DataFrame has Null values: False\n","Name of file in write method is forward_trl_dees_0\n","Final output path (write method) is /content/drive/MyDrive/mlt_final_project/evaluation_two/forward_trl_dees_0\n","Loading /content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_2\n","Loaded /content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_2\n","Unnamed: 0              0\n","Text          Reanudación\n","Lemma         Reanudación\n","Pos                 PROPN\n","Tag                 PROPN\n","Dep                  ROOT\n","Shape               Xxxxx\n","Name: 0, dtype: object\n","DataFrame has Null values: False\n","Name of file in write method is forward_trl_dees_2\n","Final output path (write method) is /content/drive/MyDrive/mlt_final_project/evaluation_two/forward_trl_dees_2\n","Loading /content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_3\n","Loaded /content/drive/MyDrive/mlt_final_project/dfs/forward_trl_dees_3\n","Unnamed: 0              0\n","Text          Reanudación\n","Lemma         Reanudación\n","Pos                 PROPN\n","Tag                 PROPN\n","Dep                  ROOT\n","Shape               Xxxxx\n","Name: 0, dtype: object\n","DataFrame has Null values: False\n","Name of file in write method is forward_trl_dees_3\n","Final output path (write method) is /content/drive/MyDrive/mlt_final_project/evaluation_two/forward_trl_dees_3\n","Loading /content/drive/MyDrive/mlt_final_project/dfs/translated.es\n","Loaded /content/drive/MyDrive/mlt_final_project/dfs/translated.es\n","Unnamed: 0              0\n","Text          Reanudación\n","Lemma         Reanudación\n","Pos                 PROPN\n","Tag                 PROPN\n","Dep                 nsubj\n","Shape               Xxxxx\n","Name: 0, dtype: object\n","DataFrame has Null values: False\n","Name of file in write method is translated.es\n","Final output path (write method) is /content/drive/MyDrive/mlt_final_project/evaluation_two/translated.es\n"]}],"source":["load_and_evaluate_df()"]},{"cell_type":"code","source":["process() # (path_obj, language, name,"],"metadata":{"id":"geAoqn8ruXUF"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"background_execution":"on","collapsed_sections":[],"name":"mlt_project_spacy_eval.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}